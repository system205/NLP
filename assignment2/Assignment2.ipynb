{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Used files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rwxrwxrwx 1 root root 6.2M Feb 28 13:40 big.txt\n",
            "-rwxrwxrwx 1 root root  64M Oct 19  2019 reviews.csv\n",
            "-rwxrwxrwx 1 root root  17M Feb 29 11:51 w2.txt\n",
            "-rwxrwxrwx 1 root root  27M Feb 28 19:33 w5.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lh w2.txt w5.txt reviews.csv big.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig's solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "file = open('big.txt').read()\n",
        "WORDS = Counter(words(file))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]).union(known(edits1(word))) or known(edits2(word)) or [word])\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# My solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import ngrams data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "n5 = {}\n",
        "n3 = {}\n",
        "n4 = {}\n",
        "# Read 5-gram file and store n-grams in dictionaries\n",
        "# And update 4-gram and 3-gram dictionaries along with 5-gram\n",
        "with open('w5.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for i in lines:\n",
        "        # Split line into count and n-gram\n",
        "        i = i.strip().split(\"\\t\")\n",
        "\n",
        "        # Identify n-grams\n",
        "        gram5 = tuple(i[1:])\n",
        "        gram4 = [gram5[1:], gram5[:-1]]\n",
        "        gram3 = [gram5[1:4], gram5[2:], gram5[:-2]]\n",
        "        \n",
        "        count = int(i[0])\n",
        "\n",
        "        # Store 5-gram in dictionary without duplicates\n",
        "        if gram5 not in n5:\n",
        "            n5[gram5] = count\n",
        "        else: raise Exception(\"Duplicate ngram\")\n",
        "\n",
        "\n",
        "        # Update 4-gram dictionary\n",
        "        for ngram in gram4:\n",
        "            if ngram not in n4:\n",
        "                n4[ngram] = count\n",
        "            else: \n",
        "                n4[ngram] += count\n",
        "\n",
        "        # Update 3-gram dictionary\n",
        "        for ngram in gram3:\n",
        "            if ngram not in n3:\n",
        "                n3[ngram] = count\n",
        "            else: \n",
        "                n3[ngram] += count\n",
        "            \n",
        "\n",
        "n2 = {}\n",
        "# Read 2-gram file\n",
        "with open('w2.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "    for i in lines:\n",
        "        i = i.strip().split(\"\\t\")\n",
        "        ngram = tuple(i[1:])\n",
        "        count = int(i[0])\n",
        "\n",
        "        # Store 2-gram in dictionary without duplicates\n",
        "        if ngram not in n2:\n",
        "            n2[ngram] = count\n",
        "        else: raise Exception(\"Duplicate ngram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check imported data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2-gram : 1020368\n",
            "Most common n-grams:\n",
            "[(('of', 'the'), 2586813), (('in', 'the'), 2043262), (('to', 'the'), 1055301), (('on', 'the'), 920079), (('and', 'the'), 737714)]\n",
            "\n",
            "3-gram : 619624\n",
            "Most common n-grams:\n",
            "[(('i', 'do', \"n't\"), 193377), (('a', 'lot', 'of'), 167920), (('one', 'of', 'the'), 159272), (('the', 'united', 'states'), 144059), (('do', \"n't\", 'know'), 113219)]\n",
            "\n",
            "4-gram : 999232\n",
            "Most common n-grams:\n",
            "[(('the', 'end', 'of', 'the'), 53263), (('do', \"n't\", 'want', 'to'), 50903), (('i', 'do', \"n't\", 'think'), 50304), (('i', 'do', \"n't\", 'know'), 42473), (('in', 'the', 'united', 'states'), 39050)]\n",
            "\n",
            "5-gram : 1044268\n",
            "Most common n-grams:\n",
            "[(('at', 'the', 'end', 'of', 'the'), 13588), (('i', 'do', \"n't\", 'want', 'to'), 12744), (('in', 'the', 'middle', 'of', 'the'), 9124), (('i', 'do', \"n't\", 'know', 'what'), 8076), (('you', 'do', \"n't\", 'have', 'to'), 7186)]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "grams = [n2, n3, n4, n5]\n",
        "names = [\"2-gram\", \"3-gram\", \"4-gram\", \"5-gram\"]\n",
        "\n",
        "for i in range(4):\n",
        "    print(names[i], \":\", len(grams[i]))\n",
        "    print(\"Most common n-grams:\")\n",
        "    print(Counter(grams[i]).most_common(5))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions to consider context of word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_by_context(word, context):\n",
        "    '''Find the word in the ngram context\n",
        "    Returns the context length and the count of the word occurences for each found ngram contexts\n",
        "    First context length are longer\n",
        "    context: list of words before the word to find'''\n",
        "    context = context[-4:] # for maximum 5-gram\n",
        "\n",
        "    found_contexts = []\n",
        "    ns = [None, n2, n3, n4, n5]\n",
        "    for context_len in range(len(context), 0, -1):\n",
        "        # ngram to number of occurences\n",
        "        ngrams = ns[context_len]\n",
        "        context = context[-context_len:]\n",
        "\n",
        "        # find the word in the ngram context\n",
        "        # return context len and the count of the word occurences in such ngram\n",
        "        if (*context, word) in ngrams:\n",
        "            # compact the context count\n",
        "            if (len(found_contexts) > 0 and found_contexts[-1][0] == context_len):\n",
        "                found_contexts[-1] = (context_len, found_contexts[-1][1] + ngrams[(*context, word)])\n",
        "            else:\n",
        "                found_contexts.append((context_len, ngrams[(*context, word)]))\n",
        "    \n",
        "    return found_contexts    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[(1, 275367)], [(1, 11787)], [(2, 289), (1, 2586813)], [(1, 1927)]]\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from functools import cmp_to_key\n",
        "\n",
        "# comparator for the context\n",
        "def compare_contexts(c1, c2):\n",
        "    '''Compare two contexts\n",
        "    Return 0 if they are equal\n",
        "    Return >0 if c2 has longer context length or higher count\n",
        "    '''\n",
        "\n",
        "    if (len(c1) == 0 and len(c2) == 0):\n",
        "        return 0\n",
        "    elif len(c1) == 0:\n",
        "        return -1\n",
        "    elif len(c2) == 0:\n",
        "        return 1\n",
        "    \n",
        "    # compare by context length\n",
        "    if c1[0][0] != c2[0][0]:\n",
        "        return c1[0][0] - c2[0][0]\n",
        "    # compare by count\n",
        "    return c1[0][1] - c2[0][1]\n",
        "\n",
        "def find_max_contex_index(contexts):\n",
        "    '''Find the max context in the list of contexts'''\n",
        "    max_context = max(contexts, key=cmp_to_key(compare_contexts))\n",
        "    # find index of the max context\n",
        "    max_context_index = contexts.index(max_context)\n",
        "    return max_context_index\n",
        "\n",
        "# Test the function\n",
        "c1 = find_by_context(\"the\", ['is'])\n",
        "c2 = find_by_context(\"he\", ['is'])\n",
        "c3 = find_by_context(\"the\", ['is','of'])\n",
        "c4 = find_by_context(\"the\", ['is', 'of', 'the'])\n",
        "c = [c1, c2, c3, c4]\n",
        "print(c)\n",
        "print(find_max_contex_index(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Wrong word: e\n",
            "Candidates: ['ex', 'es', 'em', 'er', 'oe', 'ed', 'w', 'eh', 'o', 'le', 'a', 're', 've', 'be', 'd', 'te', 'ne', 'z', 'ye', 'c', 'he', 'j', 'u', 'ce', 'p', 'en', 'se', 'fe', 'et', 'el', 'eg', 'm', 'k', 'b', 'i', 'q', 't', 'ke', 's', 'e', 'me', 'we', 'v', 'ev', 'g', 'f', 'n', 'de', 'y', 'ze', 'x', 'je', 'pe', 'l', 'r', 'h']\n",
            "Best context index: 13\n",
            "Top context len: 2 with occurences 613\n",
            "be\n",
            "\n",
            "Wrong word: he\n",
            "Candidates: ['hen', 'oe', 'eh', 'hem', 'le', 're', 'hue', 'be', 've', 'te', 'ne', 'ye', 'hey', 'he', 'her', 'ce', 'se', 'she', 'fe', 'ha', 'hi', 'hm', 'ho', 'the', 'ke', 'e', 'me', 'we', 'heh', 'de', 'ze', 'je', 'pe', 'hew', 'h']\n",
            "Best context index: 23\n",
            "Top context len: 4 with occurences 564\n",
            "the\n",
            "\n",
            "Wrong word: yo\n",
            "Candidates: ['yon', 'vo', 'o', 'to', 'co', 'go', 'no', 'mo', 'wo', 'you', 'do', 'myo', 'ye', 'lo', 'oo', 'po', 'so', 'y', 'fo', 'ho']\n",
            "Best context index: 3\n",
            "Top context len: 2 with occurences 39398\n",
            "to\n",
            "\n",
            "Wrong word: yo\n",
            "Candidates: ['yon', 'vo', 'o', 'to', 'co', 'go', 'no', 'mo', 'wo', 'you', 'do', 'myo', 'ye', 'lo', 'oo', 'po', 'so', 'y', 'fo', 'ho']\n",
            "Best context index: 9\n",
            "Top context len: 2 with occurences 908\n",
            "you\n",
            "\n",
            "Wrong word: yo\n",
            "Candidates: ['yon', 'vo', 'o', 'to', 'co', 'go', 'no', 'mo', 'wo', 'you', 'do', 'myo', 'ye', 'lo', 'oo', 'po', 'so', 'y', 'fo', 'ho']\n",
            "Best context index: 9\n",
            "Top context len: 4 with occurences 19\n",
            "you\n",
            "\n",
            "Wrong word: he\n",
            "Candidates: ['hen', 'oe', 'eh', 'hem', 'le', 're', 'hue', 'be', 've', 'te', 'ne', 'ye', 'hey', 'he', 'her', 'ce', 'se', 'she', 'fe', 'ha', 'hi', 'hm', 'ho', 'the', 'ke', 'e', 'me', 'we', 'heh', 'de', 'ze', 'je', 'pe', 'hew', 'h']\n",
            "Best context index: 7\n",
            "Top context len: 1 with occurences 83595\n",
            "be\n",
            "\n",
            "Wrong word: he\n",
            "Candidates: ['hen', 'oe', 'eh', 'hem', 'le', 're', 'hue', 'be', 've', 'te', 'ne', 'ye', 'hey', 'he', 'her', 'ce', 'se', 'she', 'fe', 'ha', 'hi', 'hm', 'ho', 'the', 'ke', 'e', 'me', 'we', 'heh', 'de', 'ze', 'je', 'pe', 'hew', 'h']\n",
            "Best context index: 23\n",
            "Top context len: 4 with occurences 8\n",
            "the\n",
            "\n",
            "Wrong word: he\n",
            "No found context. Get most probable correction\n",
            "the\n"
          ]
        }
      ],
      "source": [
        "def correct_by_context(wrong, context, verbose=False):\n",
        "    '''Correct the word by the context\n",
        "    for each candidate search for the context and choose the one with the longest context or the highest count\n",
        "    '''\n",
        "    if verbose:\n",
        "        print()\n",
        "        print('Wrong word:', wrong)\n",
        "\n",
        "    cands = list(candidates(wrong))\n",
        "    found_contexts = []\n",
        "    for c in cands:\n",
        "        found_contexts.append(find_by_context(c, context))\n",
        "\n",
        "    if all([len(f) == 0 for f in found_contexts]): \n",
        "        if verbose: print('No found context. Get most probable correction')\n",
        "        return max(cands, key=P)\n",
        "        \n",
        "    best_context_index = find_max_contex_index(found_contexts)\n",
        "    if verbose:\n",
        "        print(\"Candidates:\", cands)\n",
        "        print(\"Best context index:\", best_context_index)\n",
        "        best_context = found_contexts[best_context_index]\n",
        "        if (len(best_context) > 0):\n",
        "            best_context = best_context[0]\n",
        "            print(f'Top context len: {best_context[0]} with occurences {best_context[1]}')\n",
        "    return cands[best_context_index]\n",
        "\n",
        "# Test the function\n",
        "print(correct_by_context(\"e\", words('He could'), verbose=True))\n",
        "print(correct_by_context(\"he\", words(\"with the help of\"), verbose=True)) # good context\n",
        "print(correct_by_context(\"yo\", words('I want'), verbose=True)) # easy context\n",
        "print(correct_by_context(\"yo\", words('I state I love'), verbose=True)) # to be 2 context\n",
        "print(correct_by_context(\"yo\", words('I think I love'), verbose=True)) # 4 context\n",
        "print(correct_by_context(\"he\", words('How this film could'), verbose=True)) # good context\n",
        "print(correct_by_context(\"he\", words('I am going to a back room of'), verbose=True)) # long context (will be cut)\n",
        "print(correct_by_context(\"he\", words(''), verbose=True)) # No context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "1. Firstly, instead of returning candidates if the word is known, I return this word along with one edit distance candidates instead of returning nothing as Norvig. This is because the word might be known, but the word in the context is different. For example, \"he\" is known, but in the context \"he needs\" it might be \"the needs\". So, 'he' will always be checked as 'me', 'be', 'the' etc.\n",
        "2. Secondly, I use suggested datasets of 2 and 5 grams since it has over 1M ngrams. \n",
        "3. Then, I construct 3 and 4 grams datasets from 5grams by slicing them. I do it also for performance for fast search of 3gram context instead of all 5grams. That's why all my corrections have the same worst case time complexity as Norvig.\n",
        "4. For each candidate word I check the presence of it with some context in my dictionaries. As a final decision which word to suggest I base on the following:\n",
        "    1. The longer the context of a word the higher chances for it to be suggested. I do it since it is barely possible to overfit with the data, and long custructions such as 'with the help of' should always be prefered to smaller contexts. And context of len 1 is always prefered to popular suggestion but without contex coupling\n",
        "    2. If 2 candidate words have equal largest contexts I just look at their occurances as the higher it is the more probably in my data the word will appear\n",
        "5. In addition, I do not find separate contexts occurances of the same lengths. For a word I combine all the same length context into single tuple (context_len, n) where n is total count of occurances in the contexts with length context_len. So, the more a word appear in long contexts the more probable it is to be \"typed\" by a user as they want\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMDB reviews dataset of sentences\n",
        "reviews = pd.read_csv(\"reviews.csv\")['review'].apply(lambda t: t.lower()).apply(words)\n",
        "reviews = [r[:5] for r in reviews if r[4] != 'br'] # remove <br> tags\n",
        "reviews = reviews[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a function that takes a word and returns it with some error or noise\n",
        "def mutate_word(word):\n",
        "    '''applies some mutation on a word\n",
        "    random letter replacement\n",
        "    removal of a random letter if there are at least 2\n",
        "    insertion of a letter at the end\n",
        "    '''\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    \n",
        "    action = np.random.randint(0, 3)\n",
        "    \n",
        "    if action == 0:\n",
        "        # replace a letter\n",
        "        word = list(word)\n",
        "        word[np.random.randint(0, len(word))] = alphabet[np.random.randint(0, 26)]\n",
        "        word = ''.join(word)\n",
        "    elif action == 1 and len(word) > 1:\n",
        "        # remove a letter\n",
        "        rem_index = np.random.randint(0, len(word))\n",
        "        word = word[:rem_index] + word[rem_index+1:]\n",
        "    else:\n",
        "        # add a letter\n",
        "        word = word + alphabet[np.random.randint(0, 26)]\n",
        "\n",
        "    return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Norvig's corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64% of 5000 correct (8% unknown) at 269 words per second \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# last word is incorrect in each sentence\n",
        "test_sentences = [r[:-1] + [mutate_word(r[-1])] for r in reviews]\n",
        "good = 0\n",
        "unknown = 0\n",
        "n = len(test_sentences)\n",
        "verbose = False\n",
        "\n",
        "start = time.time()\n",
        "for sent, corr_sent in zip(test_sentences,reviews):\n",
        "    wrong = sent[-1]\n",
        "    corr = corr_sent[-1]\n",
        "    pred = correction(wrong)\n",
        "    unknown += (corr not in WORDS)\n",
        "    good += (pred == corr)\n",
        "    if verbose and pred != corr:\n",
        "        print('correction({}) => {} ({}); expected {} ({}). Context: {}'.format(wrong, pred, WORDS[pred], corr, WORDS[corr], \" \".join(sent)))\n",
        "\n",
        "dt = time.time() - start\n",
        "\n",
        "print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '.format(good / n, n, unknown / n, n / dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test my corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76% of 5000 correct (8% unknown) at 273 words per second \n"
          ]
        }
      ],
      "source": [
        "good = 0\n",
        "unknown = 0\n",
        "\n",
        "verbose = False\n",
        "\n",
        "start = time.time()\n",
        "for sent, corr_sent in zip(test_sentences,reviews):\n",
        "    wrong = sent[-1]\n",
        "    context = sent[:-1]\n",
        "    corr = corr_sent[-1]\n",
        "    pred = correct_by_context(wrong, context)\n",
        "    unknown += (corr not in WORDS)\n",
        "    good += (pred == corr)\n",
        "    if verbose and pred != corr:\n",
        "        print('correction({}) => {}; expected {}. Context: {}'.format(wrong, pred, corr, \" \".join(sent)))\n",
        "\n",
        "dt = time.time() - start\n",
        "print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '.format(good / n, n, unknown / n, n / dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('one', 'love')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Straight comparison of correction for word\n",
        "word = 'ove'\n",
        "context_sent = words('you are my')\n",
        "\n",
        "# Norvig function vs My function\n",
        "correction(word), correct_by_context(word, context_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison and accuracies\n",
        "- My solution: 76% accuracy. It is definitely not worse since if no context was familiar it will produce most common correction as Norvig. In other cases it checks for at most 4 context types (2,3,4,5-grams) in dictionaries which is not slower as well.\n",
        "- Norvig's solution: 64% accuracy. Does not use any context, sometimes it is even a bit slower on my machine ^_^ than my solution with ngram context since the complexity is nearly the same"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
